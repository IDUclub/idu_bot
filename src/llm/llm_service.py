from src.common.config.config import Config


class LlmService:
    def __init__(self, config: Config):
        self.url = f"http://{config.get('LLM_HOST')}:{config.get('LLM_PORT')}"
        self.model_name = config.get('LLM_MODEL')

    async def generate_request_data(self, message: str, context: str) -> tuple[dict, dict]:
        data = {
            "model": self.model_name,
            "prompt": f"НАЧАЛО ВОПРОСА | {message} | КОНЕЦ ВОПРОСА",
            "stream": True,
            "system": "Ты отвечаешь на вопросы по документам, связанным с градостроительством и урбанистикой \"Региональные нормативы градостроительного проектирования Ленинградской области\" (или РНГП), \"ОБ УТВЕРЖДЕНИИ СТРАТЕГИИ СОЦИАЛЬНО-ЭКОНОМИЧЕСКОГО РАЗВИТИЯМУНИЦИПАЛЬНОГО ОБРАЗОВАНИЯ ГОРОД ПЕРМЬ ДО 2030 ГОДА\" или Соцстратегия Перми, \"О СТРАТЕГИИ СОЦИАЛЬНО-ЭКОНОМИЧЕСКОГО РАЗВИТИЯ САНКТ-ПЕТЕРБУРГА НА ПЕРИОД ДО 2035 ГОДА\" или соцстратегия СПБ"
                      "инструкция: Ответь на вопрос на основе документа."
                      "Если он не подходит, скажи об этом. Если в тексте не было вопроса или просьбы, попроси уточнить запрос."
                      "Отвечай вежливо. Отвечай только на русском языке."
                      "Если с тобой здороваются, здоровайся в ответ. Если тебя спрашивают, что ты умеешь делать,"
                      "отвечай, что ты анализируешь документы и отвечаешь на вопросы по ним.\n"
                      f"НАЧАЛО ДОКУМЕНТА | {context} | КОНЕЦ ДОКУМЕНТА",
            "options": {
                "temperature": 0.2
            }
        }
        print(context)
        headers = {
            "Content-Type": "application/json"
        }
        return headers, data
